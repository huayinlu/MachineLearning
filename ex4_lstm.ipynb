{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入所需库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如缺少 gensim库则按以下步骤安装\n",
    "根据gensim的版本不同（此处为3.6.0），初始化word2vec模型的时候，形参名称不同，，\n",
    "最新版详细信息请参考\n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py\n",
    "版本自查:\n",
    "import gensim\n",
    "gensim.__version__\n",
    "     \n",
    "安装3.6.0版本\n",
    "pip uninstall gensim\n",
    "pip install gensim==3.6.0\n",
    "安装后重启环境\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gensim.models import word2vec\n",
    "student_id = 3218001578 # 填写学号\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一些函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(path='training_label.txt'):\n",
    "    '''\n",
    "    原始文件可以使用 notepad++软件预览\n",
    "    读取训练数据\n",
    "    training_label.txt:带label，需要读label\n",
    "    training_nolabel.txt 则无需读label\n",
    "    '''\n",
    "    if 'training_label' in path:\n",
    "        with open(path, 'r',encoding='utf-8') as f: \n",
    "            '''\n",
    "            如出现：\n",
    "            UnicodeDecodeError: 'gbk' codec can't decode byte 0xb9 in position 7958: illegal multibyte sequence\n",
    "            去掉：\n",
    "            encoding='utf-8'\n",
    "            '''\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\n').split(' ') for line in lines] #  对每一行sentence移除换行符 '\\n' 并以空白符' '所在位置分割单词\n",
    "        x = [line[2:] for line in lines]\n",
    "        y = [line[0] for line in lines] # 每句 句首 是标签0或1\n",
    "        return x, y\n",
    "    else:\n",
    "        with open(path, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            x = [line.strip('\\n').split(' ') for line in lines]\n",
    "        return x\n",
    "\n",
    "def load_testing_data(path='testing_data'):\n",
    "    # 把testing時需要的data讀進來\n",
    "    with open(path, 'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]   #移除换行符并以'，'所在位置分割，将前面的序号剔除\n",
    "        X = [sen.split(' ') for sen in X]\n",
    "    return X\n",
    "\n",
    "# 验证集\n",
    "def evaluation(outputs, labels):\n",
    "    #outputs => probability (float)\n",
    "    #labels => labels\n",
    "    outputs[outputs>=0.5] = 1 # 大于等于0.5为有恶意\n",
    "    outputs[outputs<0.5] = 0 \n",
    "    correct = torch.sum(torch.eq(outputs, labels)).item() # 计数\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练 word embedding 模块\n",
    "该模块默认在 cpu 上训练，需要大概10分钟\n",
    "\n",
    "生成以下三个文件\n",
    "w2v_all.model\n",
    "w2v_all.model.trainables.syn1neg.npy\n",
    "w2v_all.model.wv.vectors.npy\n",
    "\n",
    "使用时调用w2v_all.model即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def train_word2vec(sentences):\n",
    "    # 训练word to vector 的 word embedding\n",
    "    model = word2vec.Word2Vec(sentences, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)# 词向量维度=250，迭代10次\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"loading training data ...\")\n",
    "    train_x, y = load_training_data('./data/training_label.txt')\n",
    "    train_x_no_label = load_training_data('./data/training_nolabel.txt')\n",
    "\n",
    "    print(\"loading testing data ...\")\n",
    "    test_x = load_testing_data('./data/testing_data.txt')\n",
    "\n",
    "    model = train_word2vec(train_x + train_x_no_label + test_x)\n",
    "    \n",
    "    print(\"saving model ...\")\n",
    "    os.makedirs('./model',exist_ok=True)\n",
    "    model.save(os.path.join('./model/w2v_all.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义数据处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path='./model/word2vec.model'):\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "        \n",
    "    def get_w2v_model(self):\n",
    "        ''' \n",
    "        加载训练好的word2vec.model \n",
    "        并为其随机初始化一个 向量描述子\n",
    "        '''\n",
    "        # 把之前训练好的 word to vec 模型读进来\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "        \n",
    "    def add_embedding(self, word):\n",
    "        ''' \n",
    "        添加起始、终止符: <PAD>,<UNK>\n",
    "        <PAD>,<UNK>的representation vector 由随机生成\n",
    "        '''\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "        \n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        if load:\n",
    "            print(\"loading word to vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        # 定义一个 word2idx 的 dictionary\n",
    "        # 定义一个 idx2word 的 list\n",
    "        # 定义一个 word2vector 的 list\n",
    "        \n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "#             print('get words #{}'.format(i+1), end='\\r')\n",
    "            #e.g. self.word2index['he'] = 1 \n",
    "            #e.g. self.index2word[1] = 'he'\n",
    "            #e.g. self.vectors[1] = 'he' vector\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "#         print('')\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        # 将 \"<PAD>\" 和 \"<UNK>\" 加进 embedding 里面\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "    \n",
    "    def pad_sequence(self, sentence):\n",
    "        '''统一句子长度'''\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "    \n",
    "    def sentence_word2idx(self):\n",
    "        # 把句子里面的字转成相应的index\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "#             print('sentence count #{}'.format(i+1), end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "            # 将每个句子变成一样的长度\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        print(\"total sentences: {}\".format(len(sentence_list)))\n",
    "        return torch.LongTensor(sentence_list)\n",
    "    \n",
    "    def labels_to_tensor(self, y):\n",
    "        # 把 labels 转成 tensor\n",
    "        y = [int(label) for label in y]\n",
    "        return torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义dataset\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class TwitterDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.data[idx]\n",
    "        return self.data[idx], self.label[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        #  embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        # 是否 embedding fix固定住，如果fix_embedding为False，在训练过程中，embedding也会跟着训练\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
    "                                         nn.Linear(hidden_dim, 1),\n",
    "                                         nn.Sigmoid() )\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # x 的 dimension (batch, seq_len, hidden_size)\n",
    "        # 取用 LSTM 最后一层的 hidden state\n",
    "        x = x[:, -1, :] \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
    "    \n",
    "    model.train() \n",
    "    criterion = nn.BCELoss() #binary cross entropy loss\n",
    "    t_batch = len(train) \n",
    "    v_batch = len(valid) \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss, total_acc = 0, 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train):\n",
    "            inputs = inputs.to(device, dtype=torch.long) # device為\"cuda\"，將inputs轉成torch.cuda.LongTensor\n",
    "            labels = labels.to(device, dtype=torch.float) # device為\"cuda\"，將labels轉成torch.cuda.FloatTensor，因為等等要餵進criterion，所以型態要是float\n",
    "            \n",
    "            optimizer.zero_grad() # loss.backward()的gradient会累加，每训练一个epoch需要清零\n",
    "            outputs = model(inputs) \n",
    "            outputs = outputs.squeeze() # 维度压缩，去掉dim为1的维度，使得outputs 和labels的维度匹配，能够进行criterion()\n",
    "            loss = criterion(outputs, labels) \n",
    "            loss.backward() # 自动梯度\n",
    "            optimizer.step() # 模型参数更新\n",
    "            correct = evaluation(outputs, labels) # 计算此刻模型的training accuracy\n",
    "            total_acc += (correct / batch_size)\n",
    "            total_loss += loss.item()\n",
    "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
    "            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
    "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
    "\n",
    "        # validation\n",
    "        model.eval() # eval()模式，固定model的参数\n",
    "        with torch.no_grad(): # 设置不计算梯度\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (inputs, labels) in enumerate(valid):\n",
    "                inputs = inputs.to(device, dtype=torch.long) \n",
    "                labels = labels.to(device, dtype=torch.float) \n",
    "                \n",
    "                outputs = model(inputs) \n",
    "                outputs = outputs.squeeze() \n",
    "                loss = criterion(outputs, labels) \n",
    "                correct = evaluation(outputs, labels) \n",
    "                \n",
    "                total_acc += (correct / batch_size)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "            if total_acc > best_acc:\n",
    "                # 如果validation的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n",
    "                best_acc = total_acc\n",
    "                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
    "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
    "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
    "        print('-----------------------------------------------')\n",
    "        model.train() # 将model的模式设置为train，这样optimizer就可以更新model的参数（因为前面设置了eval模式）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "# 这个block用来对testing_data.txt做预测\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def testing(batch_size, test_loader, model, device):\n",
    "    model.eval()\n",
    "    ret_output = []\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs>=0.5] = 1 # 大于等於0.5為負面\n",
    "            outputs[outputs<0.5] = 0 # 小于0.5為正面\n",
    "            ret_output += outputs.int().tolist()\n",
    "    \n",
    "    return ret_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 通过torch.cuda.is_available()的返回值(True/False)判断GPU是否可用，True: device设为\"cuda\"，False为\"cpu\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据、w2v模型路径初始化\n",
    "data_dir = './data'\n",
    "model_dir = './model' \n",
    "train_with_label = os.path.join(data_dir, 'training_label.txt')\n",
    "train_no_label = os.path.join(data_dir, 'training_nolabel.txt')\n",
    "testing_data = os.path.join(data_dir, 'testing_data.txt')\n",
    "\n",
    "w2v_path = os.path.join(model_dir,'w2v_all.model')\n",
    "\n",
    "'''\n",
    "    训练参数初始化\n",
    "    sen_len :  句子长度\n",
    "    fix_embedding  :  是否在训练中固定embedding的大小\n",
    "    batch_size :  批次大小\n",
    "    epoch: 迭代次数\n",
    "    learning_rate:  学习率    \n",
    "'''\n",
    "\n",
    "sen_len = 30 \n",
    "fix_embedding = True # fix embedding during training\n",
    "batch_size = 32\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "print(\"loading data ...\") # 读取'training_label.txt'和'training_nolabel.txt'\n",
    "train_x, y = load_training_data(train_with_label)\n",
    "train_x_no_label = load_training_data(train_no_label)\n",
    "print(len(train_x),len(y),len(train_x_no_label))\n",
    "\n",
    "\n",
    "print(\"Dealing data ...\") # 对input 和 label 进行预处理\n",
    "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "train_x = preprocess.sentence_word2idx()\n",
    "y = preprocess.labels_to_tensor(y)\n",
    "\n",
    "\n",
    "# 定义一个model对象\n",
    "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=250, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
    "model = model.to(device) # device为\"cuda\"，model使用GPU來训练(喂进去的inputs的数据类型也需要是cuda tensor)\n",
    "\n",
    "# 把data分為training data跟validation data(將一部份training data拿去當作validation data)\n",
    "X_train, X_val, y_train, y_val = train_x[:90000], train_x[90000:], y[:90000], y[90000:]\n",
    "\n",
    "# 把data做成dataset供dataloader取用\n",
    "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
    "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
    "\n",
    "# 把data 转成 batch of tensors\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            num_workers = 0)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 0)\n",
    "\n",
    "# 开始训练\n",
    "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出现：\n",
    "RuntimeError: DataLoader worker (pid(s) 17560, 24160, 12656, 15804, 19328, 23208, 22496, 20500) exited unexpectedly\n",
    "\n",
    "更改num_worker=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用模型对测试数据做预测\n",
    "data_dir = './data'\n",
    "model_dir = './model' \n",
    "testing_data = os.path.join(data_dir, 'testing_data.txt')\n",
    "w2v_path = os.path.join(model_dir,'w2v_all.model')\n",
    "\n",
    "sen_len = 30\n",
    "fix_embedding = True # fix embedding during training\n",
    "batch_size = 128\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"loading testing data ...\")\n",
    "testing_data = os.path.join(data_dir, 'testing_data.txt')\n",
    "test_x = load_testing_data(testing_data)\n",
    "# print(len(test_x))\n",
    "# print(test_x)\n",
    "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "test_dataset = TwitterDataset(X=test_x, y=None)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 0)\n",
    "print('\\nload model ...')\n",
    "# model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
    "model = torch.load('./model/ckpt.model')\n",
    "outputs = testing(batch_size, test_loader, model, device)\n",
    "\n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
    "print(\"save csv ...\")\n",
    "os.makedirs('./submission',exist_ok=True)\n",
    "tmp.to_csv(os.path.join('./submission/submission_%s.csv'%(student_id)), index=False)\n",
    "print(\"Finish Predict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
